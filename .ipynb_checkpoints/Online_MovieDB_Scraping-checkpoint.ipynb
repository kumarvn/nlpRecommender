{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, os, sqlite3, csv, omdb, pickle, numpy as np, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Creating the movies database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect('movies_database.db') as connection:\n",
    "    c = connection.cursor()\n",
    "    c.execute(\"DROP TABLE IF EXISTS Movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create database and movies table\n",
    "with sqlite3.connect('movies_database.db') as connection:\n",
    "    c = connection.cursor()\n",
    "    c.execute(\"CREATE TABLE Movies(imdbID PRIMARY KEY, Title TEXT, Genre TEXT, Plot TEXT, Poster TEXT)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Loading the movies into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#setting the file path where the csv is present\n",
    "def loadMovies(path,file):\n",
    "    '''function requires path on computer where the csv file is stored and the name of the csv file containing\n",
    "    the title of the films for which information needs to be obtained from the omdb API. The retrieved information\n",
    "    will be inserted into the movies table belonging to the movies_database'''\n",
    "    \n",
    "    filePath = path\n",
    "    fileName = file\n",
    "    with sqlite3.connect('movies_database.db') as connection, open(os.path.join(filePath, fileName), \"rb\") as myFile:\n",
    "        c = connection.cursor()\n",
    "        myFileReader = csv.reader(myFile)\n",
    "        for movies in myFileReader:\n",
    "            r = omdb.request(t= movies[0],plot=\"full\",r='json') #using indexing since data type of movies is a list\n",
    "            movieValues = (r.json()['imdbID'],r.json()['Title'],r.json()['Genre'],r.json()['Plot'],\n",
    "                           r.json()['Poster'])\n",
    "            c.execute(\"INSERT INTO Movies VALUES(?, ?, ?, ?, ?)\",(movieValues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loadMovies('/home/clarence/Documents/theDataGeek','movielist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Creating a text file containing unique genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def uniqueGenres(path, filename):\n",
    "    '''A movie may be categorized into more than one genre, to create a corpus for each genre, we will need to first\n",
    "    extract each individual genre from the 'Genre' field in the movies table which are saved as tuples. We will then\n",
    "    proceed to retain only the unique values for genres. Lastly we will use these values to subset movie plots into\n",
    "    corpora'''\n",
    "\n",
    "    listofLists = []\n",
    "    singleList = []\n",
    "    global uniqueList\n",
    "    uniqueList = []\n",
    "    with sqlite3.connect('movies_database.db') as connection, open(os.path.join(path, filename),\n",
    "                                                                   \"wb\") as my_file:\n",
    "        my_file_writer = csv.writer(my_file,delimiter=\",\",quotechar=\"'\")\n",
    "        c = connection.cursor()\n",
    "        c.execute(\"SELECT Genre FROM Movies\")\n",
    "        for row in c.fetchall():\n",
    "            listofLists.append(row[0].split(',')) # splitting the multiple genre string into individual list items\n",
    "        for each_list in listofLists: # first level loop iterates over each list\n",
    "            for each_item in range(0,len(each_list)): # nested loop iterates over each list index\n",
    "                singleList.append(each_list[each_item]) # creates one list with duplicate genres\n",
    "        singleList = [item.strip() for item in singleList] #remove leading and trailing whitespace\n",
    "        [uniqueList.append(item) for item in singleList if item not in uniqueList] #drop duplicates\n",
    "        uniqueList.remove('N/A') # remove null values\n",
    "        uniqueList.sort()\n",
    "        \n",
    "        '''create a multidimensional numpy array with rows equal to length of uniqueList and one column.\n",
    "        Data type is specified as 16 character strings, to store each genre as a list within a list.\n",
    "        This additional step is required as the writerows function requires that csv values to be written\n",
    "        are a list of list data structure'''\n",
    "        \n",
    "        uniqueArray = np.array(range(len(uniqueList)), dtype='a16').reshape(len(uniqueList),1)\n",
    "        for item in range(0,len(uniqueList)):\n",
    "            uniqueArray[item][0] = uniqueList[item]\n",
    "        my_file_writer.writerows(uniqueArray) # write all rows at once\n",
    "        del listofLists, singleList, uniqueList #optimizing memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#running the function to create a list of unique genres\n",
    "uniqueGenres(path='/home/clarence/Documents/theDataGeek/nlpRecommender', filename='genrelist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Action']\n",
      "['Adventure']\n",
      "['Animation']\n",
      "['Biography']\n",
      "['Comedy']\n",
      "['Crime']\n",
      "['Documentary']\n",
      "['Drama']\n",
      "['Family']\n",
      "['Fantasy']\n",
      "['History']\n",
      "['Horror']\n",
      "['Music']\n",
      "['Musical']\n",
      "['Mystery']\n",
      "['Romance']\n",
      "['Sci-Fi']\n",
      "['Short']\n",
      "['Sport']\n",
      "['Thriller']\n",
      "['Western']\n"
     ]
    }
   ],
   "source": [
    "#testing if the data was sucessfully written to a csv file\n",
    "with open(os.path.join('/home/clarence/Documents/theDataGeek/nlpRecommender', 'genrelist.csv'), \"rb\") as myFile:\n",
    "    myFileReader = csv.reader(myFile)\n",
    "    for genres in myFileReader:\n",
    "        print genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Filter plots by genre and write to .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#code to manually filter plots if they contain specific genre names in the entire string\n",
    "inPath = '/home/clarence/Documents/theDataGeek/nlpRecommender'\n",
    "outPath = '/home/clarence/Documents/theDataGeek/nlpRecommender/corpora'\n",
    "fileName = 'genrelist.csv'\n",
    "\n",
    "with sqlite3.connect('movies_database.db') as connection, open(os.path.join(outPath,'Western.csv'),\n",
    "                                                               \"wb\") as outputFile:\n",
    "    c = connection.cursor() \n",
    "    my_file_writer = csv.writer(outputFile,delimiter=\",\")  \n",
    "    c.execute(\"SELECT Plot FROM Movies WHERE Genre LIKE '%Western%' \" )\n",
    "    my_file_writer.writerows(c.fetchall())\n",
    "    \n",
    "del inPath, outPath, fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''create individual corpus based on unique genres. We will use pattern search '%xxx%' where xxx is the substring\n",
    "pattern. This is the automated version of the preceeding block of code. The problem is using the iterator value\n",
    "in this case genre[0] in the LIKE clause'''\n",
    "\n",
    "inPath = '/home/clarence/Documents/theDataGeek/nlpRecommender'\n",
    "outPath = '/home/clarence/Documents/theDataGeek/nlpRecommender/corpora'\n",
    "fileName = 'genrelist.csv'\n",
    "with sqlite3.connect('movies_database.db') as connection,open(os.path.join(inPath, fileName), \"rb\") as inputFile:\n",
    "    myFileReader = csv.reader(inputFile)\n",
    "    c = connection.cursor()\n",
    "    for genre in myFileReader:\n",
    "        '''store output in files named after each genre'''\n",
    "        with open(os.path.join(outPath,'%s.csv' %genre[0]), \"wb\") as outputFile:\n",
    "            my_file_writer = csv.writer(outputFile,delimiter=\",\",quotechar=\"'\")  \n",
    "            c.execute(\"SELECT Plot FROM Movies WHERE Genre LIKE '%genre%' \" )\n",
    "            my_file_writer.writerows(c.fetchall())\n",
    "            \n",
    "del inPath, outPath, fileName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Create tokenized documents for each genre and remove punctutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenize, change to lower and remove punctuation for each plot filtered in the previous step\n",
    "from nltk import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string, nltk\n",
    "\n",
    "inPath = '/home/clarence/Documents/theDataGeek/nlpRecommender/corpora'\n",
    "outPath = '/home/clarence/Documents/theDataGeek/nlpRecommender/tokenized'\n",
    "fileName = 'genrelist.csv'\n",
    "genreList = [] #store genre names in memory from csv file\n",
    "tokenizer = WordPunctTokenizer() #instance of WordPunctTokenizer class\n",
    "lemmatizer = WordNetLemmatizer() #instance of WordNetLemmatizer class\n",
    "\n",
    "with open(os.path.join('/home/clarence/Documents/theDataGeek/nlpRecommender', fileName), \"rb\") as inputFile:\n",
    "          myFileReader = csv.reader(inputFile)\n",
    "          for i in myFileReader:\n",
    "              genreList.append(i)\n",
    "\n",
    "#creating a list of stop words and punctuations which we will remove from each document\n",
    "stop = stopwords.words('english')\n",
    "for punctuation in string.punctuation:\n",
    "    stop.append(punctuation)\n",
    "\n",
    "for index in range(0,len(genreList)):\n",
    "    with open(os.path.join(inPath, '%s.txt' %genreList[index][0]), \"r\") as inputFile,open(\n",
    "        os.path.join(outPath,'%s.txt' %genreList[index][0]), \"w\") as outputFile:\n",
    "        text = inputFile.readlines()\n",
    "        for lines in text:\n",
    "        #for lines in range(0,len(text)):\n",
    "            words = [i for i in lemmatizer.lemmatize(lines.lower())]\n",
    "            words = [i for i in tokenizer.tokenize(lines.lower()) if i not in stop]\n",
    "            for each_word in words:\n",
    "                outputFile.write(str(each_word))\n",
    "                outputFile.write('\\n')\n",
    "                \n",
    "del inPath, outPath, fileName, genreList, tokenizer, lemmatizer, text, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heroic'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('heroic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Add categories or genres to the tokenized documents and integrate them into a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
    "reader = CategorizedPlaintextCorpusReader('/home/clarence/Documents/theDataGeek/nlpRecommender/tokenized', r'.*\\.txt',\n",
    "cat_pattern=r'(\\w+)/*') # cat_pattern means use the filename excluding the extension as the category/genre name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'nick', u'fury', u'director', u'h', u'e', u'l', u'd', ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.words('Action.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Action',\n",
       " 'Adventure',\n",
       " 'Animation',\n",
       " 'Biography',\n",
       " 'Comedy',\n",
       " 'Crime',\n",
       " 'Documentary',\n",
       " 'Drama',\n",
       " 'Family',\n",
       " 'Fantasy',\n",
       " 'History',\n",
       " 'Horror',\n",
       " 'Music',\n",
       " 'Musical',\n",
       " 'Mystery',\n",
       " 'Romance',\n",
       " 'Sci',\n",
       " 'Short',\n",
       " 'Sport',\n",
       " 'Thriller',\n",
       " 'Western']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#listing all the categories in the corpus\n",
    "reader.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Western']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validating that the mapping of category to document\n",
    "reader.categories(['Western.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#conditional frequency distribution \n",
    "cfd = nltk.ConditionalFreqDist((genre, word)\n",
    "                               for genre in reader.categories()\n",
    "                               for word in reader.words(categories=genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mystery',\n",
       " 'Romance',\n",
       " 'Short',\n",
       " 'Sci',\n",
       " 'Family',\n",
       " 'Horror',\n",
       " 'Thriller',\n",
       " 'Sport',\n",
       " 'Crime',\n",
       " 'Drama',\n",
       " 'Fantasy',\n",
       " 'Western',\n",
       " 'Animation',\n",
       " 'Music',\n",
       " 'Adventure',\n",
       " 'Action',\n",
       " 'Comedy',\n",
       " 'Documentary',\n",
       " 'Musical',\n",
       " 'Biography',\n",
       " 'History']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd.conditions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Calculating chi-square value for each word genre pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a contingency table from the conditional frequency distribution to be used in a chi-square test of assoc.\n",
    "def nltk_cfd_to_pd_dataframe(cfd):\n",
    "    \"\"\" Converts an nltk.ConditionalFreqDist object into a pandas DataFrame object. \"\"\"\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame()\n",
    "    for cond in cfd.conditions():\n",
    "        col = pd.DataFrame(pd.Series(dict(cfd[cond])))\n",
    "        col.columns = [cond]\n",
    "        df = df.join(col, how = 'outer')\n",
    "\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contingency = nltk_cfd_to_pd_dataframe(cfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#writing the dataframe to disk to remove punctuation, numbers etc. which may have remained. Will try to automate\n",
    "contingency.to_csv('/home/clarence/Documents/theDataGeek/nlpRecommender/contingency.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading the cleanded dataframe back into memory\n",
    "import pandas as pd\n",
    "\n",
    "contingency = pd.read_csv('/home/clarence/Documents/theDataGeek/nlpRecommender/contingency.csv',sep='\\t',\n",
    "                          index_col='Word',skiprows=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#calculating sum of each row\n",
    "rowSums = contingency.sum(1)\n",
    "#calculating the sum of each column\n",
    "colSums= contingency.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''creating a 2130 X 21 multidimensional array to hold expected values of each cell, where\n",
    "E = (rowSum*colSum)/Total'''\n",
    "expected = np.array(range(len(rowSums)*len(colSums)),dtype='float').reshape(len(rowSums),len(colSums))\n",
    "\n",
    "from __future__ import division # division from python 3 ensures floats and not ints are returned\n",
    "\n",
    "for i in range(0,len(rowSums)):\n",
    "    for j in range(0,len(colSums)):\n",
    "        expected[i][j] = (rowSums[i]*colSums[j])/(rowSums.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''converting the array into a dataframe sharing column names and index as that of contigency, this is important\n",
    "for the mathematical operations to follow'''\n",
    "expectedDF = pd.DataFrame(expected,columns=contingency.columns,index=contingency.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''first part of the equation (Observed-Expected)^2'''\n",
    "chiSqVals = pd.DataFrame((contingency.values-expectedDF.values)**2, columns=contingency.columns,\n",
    "                         index=contingency.index)\n",
    "\n",
    "'''second part of the equation [(O-E)^2]/E'''\n",
    "chiSqVals = chiSqVals.div(expectedDF.ix[0],axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''write out the final file in csv format and delete all other intermediate dataframes stored in memory'''\n",
    "chiSqVals.to_csv('/home/clarence/Documents/theDataGeek/nlpRecommender/chiSqVals.csv',sep='\\t')\n",
    "del contingency, expectedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/clarence/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this code block is useful for downloading relevant documents required for text processing e.g. stopwords\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Calculating Genre Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the chi-square value table for genres\n",
    "import pandas as pd\n",
    "genreChi = pd.read_csv('/home/clarence/Shared/Personal/CW/theDataGeek/nlpRecommender/chiSqVals.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.342935607224\n"
     ]
    }
   ],
   "source": [
    "#the same process can be used for movie to movie similarity scores\n",
    "from scipy import spatial\n",
    "\n",
    "v1 = genreChi['Music']\n",
    "v2 = genreChi['Musical']\n",
    "result = 1 - spatial.distance.cosine(v1, v2)\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = omdb.request(t= 'The Avengers',plot='full',r='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Actors': u'Robert Downey Jr., Chris Evans, Mark Ruffalo, Chris Hemsworth',\n",
       " u'Awards': u'Nominated for 1 Oscar. Another 31 wins & 66 nominations.',\n",
       " u'Country': u'USA',\n",
       " u'Director': u'Joss Whedon',\n",
       " u'Genre': u'Action, Adventure, Sci-Fi',\n",
       " u'Language': u'English, Russian',\n",
       " u'Metascore': u'69',\n",
       " u'Plot': u\"Nick Fury is director of S.H.I.E.L.D, an international peace keeping agency. The agency is a who's who of Marvel Super Heroes, with Iron Man, The Incredible Hulk, Thor, Captain America, Hawkeye and Black Widow. When global security is threatened by Loki and his cohorts, Nick Fury and his team will need all their powers to save the world from disaster.\",\n",
       " u'Poster': u'http://ia.media-imdb.com/images/M/MV5BMTk2NTI1MTU4N15BMl5BanBnXkFtZTcwODg0OTY0Nw@@._V1_SX300.jpg',\n",
       " u'Rated': u'PG-13',\n",
       " u'Released': u'04 May 2012',\n",
       " u'Response': u'True',\n",
       " u'Runtime': u'143 min',\n",
       " u'Title': u'The Avengers',\n",
       " u'Type': u'movie',\n",
       " u'Writer': u'Joss Whedon (screenplay), Zak Penn (story), Joss Whedon (story)',\n",
       " u'Year': u'2012',\n",
       " u'imdbID': u'tt0848228',\n",
       " u'imdbRating': u'8.2',\n",
       " u'imdbVotes': u'818,776'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#example of how to use the OMDB api directly\n",
    "r1 = requests.get('http://www.omdbapi.com/?t=The+Avengers&plot=full&r=json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tt0848228'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(r1.json()['imdbID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Plot', u'Rated', u'tomatoImage', u'Title', u'DVD', u'tomatoMeter', u'Writer', u'tomatoUserRating', u'Production', u'Actors', u'tomatoFresh', u'Type', u'imdbVotes', u'Website', u'tomatoConsensus', u'Poster', u'tomatoRotten', u'Director', u'Released', u'tomatoUserReviews', u'Awards', u'Genre', u'tomatoUserMeter', u'imdbRating', u'Language', u'Country', u'BoxOffice', u'Runtime', u'tomatoReviews', u'imdbID', u'Metascore', u'Response', u'tomatoRating', u'Year']\n"
     ]
    }
   ],
   "source": [
    "movieHeaders = list(r.json().keys())\n",
    "print movieHeaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imdb import IMDb\n",
    "ia= IMDb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Plot'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieHeaders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(r.json().keys())):\n",
    "  type(r.json().values()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r.json().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(r.json().values()[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action, Adventure, Sci-Fi\n"
     ]
    }
   ],
   "source": [
    "print r.json()['Genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MovieID,Title,Genre=[str(r.json()['imdbID']),str(r.json()['Title']),str(r.json()['Genre'])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
